{
  "version": "1.0",
  "inputs": [
    {
      "type": "InferenceImage",
      "name": "image"
    }
  ],
  "steps": [
    {
      "type": "roboflow_core/roboflow_object_detection_model@v2",
      "name": "model",
      "images": "$inputs.image",
      "model_id": "official-porifera-classifier-ju8er/12"
    },
    {
      "type": "roboflow_core/bounding_box_visualization@v1",
      "name": "bounding_box_visualization",
      "image": "$inputs.image",
      "predictions": "$steps.model.predictions"
    },
    {
      "type": "roboflow_core/byte_tracker@v3",
      "name": "byte_tracker",
      "image": "$inputs.image",
      "detections": "$steps.model.predictions",
      "fps": 60,
      "track_buffer": 60,
      "track_thresh": 0.5,
      "match_thresh": 0.8
    },
    {
      "type": "roboflow_core/label_visualization@v1",
      "name": "label_visualization",
      "image": "$steps.bounding_box_visualization.image",
      "predictions": "$steps.model.predictions"
    },
    {
      "name": "detection_converter",
      "type": "Detection_Converter",
      "detection_results": "$steps.model.predictions",
      "raw_predictions": "$steps.byte_tracker.new_instances"
    },
    {
      "name": "draw_custom_label",
      "type": "Add_Webcam_Interface",
      "image": "$steps.label_visualization.image",
      "detections": "$steps.detection_converter.detections",
      "new_instances": "$steps.byte_tracker.new_instances"
    },
    {
      "name": "roboflow_dataset_upload",
      "type": "roboflow_core/roboflow_dataset_upload@v2",
      "images": "$steps.draw_custom_label.output_image",
      "target_project": "audtheia-official-database",
      "usage_quota_name": "upload_quota_RDjlO",
      "predictions": "$steps.model.predictions"
    },
    {
      "name": "anthropic_environmental_analyzer",
      "type": "Anthropic_Environmental_Analyzer",
      "detections": "$steps.detection_converter.detections",
      "image": "$steps.draw_custom_label.output_image"
    },
    {
      "name": "analyst_caller",
      "type": "Analyst_Caller",
      "anthropic_analysis": "$steps.anthropic_environmental_analyzer.anthropic_analysis"
    }
  ],
  "outputs": [
    {
      "type": "JsonField",
      "name": "output_image",
      "coordinates_system": "own",
      "selector": "$steps.draw_custom_label.output_image"
    }
  ],
  "dynamic_blocks_definitions": [
    {
      "type": "DynamicBlockDefinition",
      "manifest": {
        "type": "ManifestDescription",
        "description": "Sends ONLY Anthropic Claude analysis to N8N with intelligent result detection.",
        "block_type": "Analyst_Caller",
        "inputs": {
          "anthropic_analysis": {
            "type": "DynamicInputDefinition",
            "selector_types": [
              "input_parameter",
              "step_output"
            ],
            "selector_data_kind": {
              "input_parameter": [
                "dictionary"
              ],
              "step_output": [
                "dictionary"
              ]
            }
          }
        },
        "outputs": {}
      },
      "code": {
        "type": "PythonCode",
        "run_function_code": "import requests\nimport time\nimport threading\nfrom typing import Any, Dict\nfrom concurrent.futures import ThreadPoolExecutor\n\n# === N8N CONFIGURATION ===\nN8N_WEBHOOK_URL = \"[YOUR-WEBHOOK-URL-HERE]\"\nHTTP_TIMEOUT = 5\nMAX_HTTP_THREADS = 2\n\nclass SilentN8NCommunicator:\n    \"\"\"Silent N8N communicator with aggressive transmission logic\"\"\"\n    \n    def __init__(self):\n        self.frame_counter: int = 0\n        self.total_transmissions: int = 0\n        self.http_executor = ThreadPoolExecutor(max_workers=MAX_HTTP_THREADS, thread_name_prefix=\"N8N\")\n    \n    def should_transmit(self, analysis_text: str) -> bool:\n        \"\"\"Detect any meaningful analysis for transmission\"\"\"\n        \n        if not analysis_text or len(analysis_text) < 50:\n            return False\n        \n        # Accept comprehensive analysis (real Claude OR quality fallback)\n        quality_indicators = [\n            \"claude_environmental_analysis\",\n            \"scientifically_validated\", \n            \"audtheia_environmental_monitoring\",\n            \"Species Identification\",\n            \"Environmental Conditions\",\n            \"Habitat Assessment\",\n            \"Conservation Implications\",\n            \"background_processing_complete\"\n        ]\n        \n        # Reject only basic interim responses\n        reject_patterns = [\n            \"awaiting_claude_analysis\",\n            \"environmental_monitoring_active\"\n        ]\n        \n        has_quality = any(indicator in analysis_text for indicator in quality_indicators)\n        has_reject = any(pattern in analysis_text for pattern in reject_patterns)\n        \n        # Accept if has quality indicators and no reject patterns\n        return has_quality and not has_reject\n    \n    def transmit_to_n8n(self, analysis_text: str, current_time: float):\n        \"\"\"Fire-and-forget transmission to N8N\"\"\"\n        \n        payload = {\n            \"timestamp\": current_time,\n            \"analysis\": analysis_text,\n            \"source\": \"audtheia_environmental_analysis\", \n            \"frame_number\": self.frame_counter,\n            \"system\": \"audtheia_airw\",\n            \"scientific_grade\": True,\n            \"description\": f\"Audtheia environmental analysis - Frame {self.frame_counter}\",\n            \"metadata\": {\n                \"analysis_timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(current_time)),\n                \"total_transmissions\": self.total_transmissions,\n                \"analysis_length\": len(analysis_text),\n                \"processing_method\": \"claude_environmental_analysis\"\n            }\n        }\n        \n        # Submit to thread pool for immediate return\n        self.http_executor.submit(self._execute_transmission, payload)\n        self.total_transmissions += 1\n\n    def _execute_transmission(self, payload: Dict):\n        \"\"\"Execute HTTP transmission silently\"\"\"\n        try:\n            response = requests.post(\n                N8N_WEBHOOK_URL,\n                json=payload,\n                timeout=HTTP_TIMEOUT,\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"User-Agent\": \"Audtheia-AIRW/3.0\"\n                }\n            )\n            # Silent operation - no console output\n        except:\n            # Silent error handling\n            pass\n\n# Global communicator\n_communicator = SilentN8NCommunicator()\n\ndef run(self, anthropic_analysis: Any) -> Dict:\n    \"\"\"\n    SILENT ANALYST CALLER\n    Transmits comprehensive analysis to N8N with minimal console output\n    \"\"\"\n    global _communicator\n    \n    _communicator.frame_counter += 1\n    current_time = time.time()\n    \n    try:\n        # Extract analysis text\n        if isinstance(anthropic_analysis, dict):\n            analysis_text = anthropic_analysis.get(\"anthropic_analysis\", \"\")\n        else:\n            analysis_text = str(anthropic_analysis) if anthropic_analysis else \"\"\n        \n        # Transmit if meaningful analysis detected\n        if _communicator.should_transmit(analysis_text):\n            _communicator.transmit_to_n8n(analysis_text, current_time)\n        \n        return {}\n        \n    except:\n        return {}"
      }
    },
    {
      "type": "DynamicBlockDefinition",
      "manifest": {
        "type": "ManifestDescription",
        "description": "Uses Anthropic Claude to analyze environmental context with true asynchronous processing.",
        "block_type": "Anthropic_Environmental_Analyzer",
        "inputs": {
          "detections": {
            "type": "DynamicInputDefinition",
            "selector_types": [
              "input_parameter",
              "step_output"
            ],
            "selector_data_kind": {
              "input_parameter": [
                "dictionary"
              ],
              "step_output": [
                "dictionary"
              ]
            }
          },
          "image": {
            "type": "DynamicInputDefinition",
            "selector_types": [
              "input_image",
              "step_output_image"
            ],
            "selector_data_kind": {
              "input_image": [
                "image"
              ],
              "step_output_image": [
                "image"
              ]
            }
          }
        },
        "outputs": {
          "anthropic_analysis": {
            "type": "DynamicOutputDefinition",
            "kind": [
              "dictionary"
            ]
          }
        }
      },
      "code": {
        "type": "PythonCode",
        "run_function_code": "import requests\nimport base64\nimport time\nimport cv2\nimport threading\nimport numpy as np\nfrom typing import Any, Dict, Optional, List\nfrom inference.core.workflows.execution_engine.entities.base import WorkflowImageData\n\n# === ANTHROPIC API CONFIGURATION ===\nANTHROPIC_API_KEY = \"[YOUR-API-KEY-HERE]\"\nANTHROPIC_API_URL = \"https://api.anthropic.com/v1/messages\"\n\n# === PROCESSING CONFIGURATION ===\nANALYSIS_INTERVAL_SECONDS = 15.0\nMAX_CONCURRENT_THREADS = 1  # Reduced to prevent API overload\nCLAUDE_IMAGE_SIZE = 800  # Reduced size to prevent API issues\nAPI_TIMEOUT_SECONDS = 20\n\n# === THREAD-SAFE STATE ===\nstatelock = threading.RLock()\n\nclass SilentClaudeProcessor:\n    def __init__(self):\n        self.frame_counter: int = 0\n        self.last_analysis_time: float = 0.0\n        self.latest_claude_result: str = None\n        self.active_threads: int = 0\n        self.processed_hashes: set = set()\n\n    def should_start_analysis(self, current_time: float) -> bool:\n        with statelock:\n            if self.active_threads >= MAX_CONCURRENT_THREADS:\n                return False\n            time_since_last = current_time - self.last_analysis_time\n            return time_since_last >= ANALYSIS_INTERVAL_SECONDS\n\n    def update_result(self, result: str, timestamp: float):\n        with statelock:\n            self.latest_claude_result = result\n            self.last_analysis_time = timestamp\n\n    def get_latest_result(self) -> Optional[str]:\n        with statelock:\n            return self.latest_claude_result\n\n# Global processor instance\n_processor = SilentClaudeProcessor()\n\ndef convertimage_to_base64_fixed(image: WorkflowImageData) -> Optional[str]:\n    \"\"\"Fixed image conversion with proper error handling\"\"\"\n    try:\n        # Extract numpy array from different possible formats\n        if hasattr(image, 'numpy_image') and image.numpy_image is not None:\n            img_array = image.numpy_image\n        elif hasattr(image, 'data') and image.data is not None:\n            img_array = image.data\n        else:\n            return None\n        \n        if img_array is None or len(img_array.shape) != 3:\n            return None\n        \n        # Convert RGBA to RGB if needed\n        if img_array.shape[2] == 4:\n            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2RGB)\n        elif img_array.shape[2] != 3:\n            return None\n        \n        # Ensure uint8\n        if img_array.dtype != np.uint8:\n            img_array = img_array.astype(np.uint8)\n        \n        # Resize for API compatibility\n        height, width = img_array.shape[:2]\n        if height > CLAUDE_IMAGE_SIZE or width > CLAUDE_IMAGE_SIZE:\n            if height > width:\n                new_height = CLAUDE_IMAGE_SIZE\n                new_width = int(CLAUDE_IMAGE_SIZE * width / height)\n            else:\n                new_width = CLAUDE_IMAGE_SIZE  \n                new_height = int(CLAUDE_IMAGE_SIZE * height / width)\n            \n            img_array = cv2.resize(img_array, (new_width, new_height), interpolation=cv2.INTER_AREA)\n        \n        # Encode to JPEG\n        encode_params = [cv2.IMWRITE_JPEG_QUALITY, 85]\n        success, buffer = cv2.imencode('.jpg', img_array, encode_params)\n        \n        if not success:\n            return None\n        \n        # Convert to base64\n        image_bytes = buffer.tobytes()\n        return base64.b64encode(image_bytes).decode('utf-8')\n        \n    except Exception:\n        return None\n\ndef executeclaude_api_call(image: WorkflowImageData, class_names: List[str], \n                           confidences: List[float], current_time: float) -> str:\n    \"\"\"Execute Claude API call with enhanced environmental location intelligence\"\"\"\n    \n    # Convert image to base64\n    image_b64 = convertimage_to_base64_fixed(image)\n    if not image_b64:\n        raise ValueError(\"Image conversion failed\")\n    \n    # Prepare species context\n    if class_names:\n        species_context = f\"{len(class_names)} organisms detected: {', '.join(class_names[:3])}\"\n    else:\n        species_context = \"No organisms detected in current frame\"\n    \n    # ENHANCED CLAUDE PROMPT FOR SYSTEMATICS PHENOLOGIST AI AGENT (SPAI) INTEGRATION\n    prompt = f\"\"\"You are operating as a PhD-level environmental biologist and taxonomist analyzing environmental monitoring footage for the Audtheia Project's global biodiversity surveillance network. Your analysis will be processed by the Systematics Phenologist AI Agent (SPAI) within the RTSP Analyst N8N Workflow to populate specific columns in the Species Observations Airtable database with research-grade precision.\n\n**DETECTION CONTEXT:** {species_context}\n\n**MISSION-CRITICAL DIRECTIVE:** \nYour analysis must provide exact terminology matching Airtable database columns to prevent downstream AI agent hallucinations. Every selection must be based on observable visual evidence combined with established species ecology.\n\n**DYNAMIC HABITAT CLASSIFICATION - PRIMARY ANALYSIS:**\nDetermine the primary habitat type through systematic visual assessment: Marine, Freshwater, Estuarine, Terrestrial, Mixed, or Unknown\n\n**COMPREHENSIVE ENVIRONMENTAL ANALYSIS BY HABITAT TYPE:**\n\n**MARINE ENVIRONMENT ANALYSIS** (if applicable):\n- Water column assessment: clarity (crystal clear/clear/turbid/murky), color variations, depth indicators, visibility range\n- Substrate characterization: coral formations, sand composition (fine/coarse/carbonate), rock types, algal coverage, sediment patterns\n- Ecosystem classification: coral reefs (fringing/barrier/patch), kelp forests, rocky intertidal zones, open ocean pelagic, seagrass beds, mangrove systems\n- Depth zone indicators: shallow tropical (<10m), mid-depth temperate (10-50m), deep-water characteristics (>50m)\n- Current/flow dynamics: wave action, tidal influences, water movement patterns, circulation indicators\n\n**TERRESTRIAL ENVIRONMENT ANALYSIS** (if applicable):\n- Vegetation structure: canopy coverage percentage, understory density, vertical stratification, species composition\n- Topographic features: elevation indicators, slope characteristics, aspect, drainage patterns, microhabitat variation\n- Seasonal phenological indicators: leaf condition (emerging/mature/senescent), flowering status, fruiting evidence, dormancy signs\n- Substrate characteristics: soil exposure, leaf litter depth, rock formations, ground cover composition, moisture indicators\n- Ecosystem classification: deciduous forest, coniferous forest, mixed forest, grassland prairie, savanna, tundra, desert scrubland, agricultural landscape, urban green space\n\n**FRESHWATER ENVIRONMENT ANALYSIS** (if applicable):\n- Hydrological characteristics: flow velocity, water clarity, depth variation, seasonal indicators, temperature cues\n- Ecosystem classification: rivers (fast/slow flowing), streams, lakes (oligotrophic/eutrophic), ponds, wetlands, marshes, swamps, riparian zones\n- Substrate analysis: rocky bottom, sandy substrate, muddy sediment, organic debris, aquatic vegetation presence\n- Water quality indicators: algal presence, turbidity, color, surface conditions\n\n**MIXED/TRANSITIONAL ENVIRONMENT ANALYSIS** (if applicable):\n- Ecotone characteristics: habitat boundary definition, species overlap zones, transition gradients\n- Coastal interfaces: beach/dune systems, rocky shores, estuarine mixing zones\n- Riparian corridors: stream-terrestrial interfaces, floodplain characteristics, wetland edges\n\n**SPECIES-SPECIFIC BEHAVIORAL ANALYSIS (MANDATORY EXACT TERMINOLOGY):**\nFor EACH species observed, provide precise selections based on observable behavioral evidence:\n\n**Activity Period** (mandatory - select exactly 1): Diurnal, Nocturnal, Crepuscular, Unknown\n- Base selection on observation timing, species ecology, and visible activity patterns\n- Consider species-specific circadian preferences and environmental cues\n\n**Behavioral Context** (mandatory - select exactly 1): Feeding, Resting, Social, Sessile, Reproductive, Territorial, Migration, Invasive Species\n- Feeding: foraging behavior, prey capture, feeding postures, food manipulation\n- Resting: stationary positions, reduced activity, roosting behavior, comfort behaviors\n- Social: group interactions, communication displays, cooperative behaviors, aggregation patterns\n- Sessile: permanently attached organisms (corals, sponges, barnacles)\n- Reproductive: courtship displays, mating behavior, nesting activity, parental care\n- Territorial: aggressive displays, boundary defense, resource guarding\n- Migration: directional movement, seasonal positioning, transient behavior\n- Invasive Species: non-native species identification with disruption indicators\n\n**Circadian Phase** (mandatory - select exactly 1): Active, Inactive, Transitional, Peak Activity, Unknown\n- Active: engaged in normal behavioral activities, alert, responsive\n- Inactive: reduced activity, minimal movement, energy conservation mode\n- Transitional: changing between activity states, preparation behaviors\n- Peak Activity: maximum energy behaviors, intense feeding/reproductive activity\n\n**PHENOLOGICAL ASSESSMENT (MANDATORY EXACT TERMINOLOGY):**\nBase selections on observation date, visual life stage evidence, and species-specific reproductive ecology:\n\n**Seasonal Timing** (mandatory - select exactly 1): Expected, Early, Late, Unusual, Unknown\n- Expected: behavior/life stage matches typical seasonal patterns for species\n- Early: phenological event occurring ahead of typical timing\n- Late: phenological event occurring behind typical timing\n- Unusual: atypical behavior or life stage for the season/location\n\n**Life Cycle Stage** (mandatory - select exactly 1): Juvenile, Adult, Reproductive, Migrating, Dormant, Unknown\n- Juvenile: immature individuals, subadult characteristics, growth phase indicators\n- Adult: mature individuals, full size development, adult coloration/characteristics\n- Reproductive: breeding condition indicators, spawning behavior, parental characteristics\n- Migrating: transitional movement, seasonal positioning, directional behavior\n- Dormant: reduced activity, overwintering, estivation, minimal metabolic activity\n\n**Breeding Season** (mandatory - select exactly 1): Pre-Breeding, Breeding, Post-breeding, Non-breeding, Unknown\n- Pre-Breeding: courtship preparation, territory establishment, pre-spawning conditioning\n- Breeding: active reproduction, spawning, nesting, mating displays\n- Post-breeding: parental care, juvenile rearing, post-reproductive recovery\n- Non-breeding: outside reproductive season, non-reproductive social behaviors\n\n**TAXONOMIC PRECISION REQUIREMENTS:**\n- Species identification: Provide genus and species (binomial nomenclature) when confidence is high (>80%)\n- Family-level classification: Always provide family assignment with morphological justification\n- Morphological evidence: List 3-5 specific observable characteristics supporting identification\n- Confidence assessment: Provide numerical confidence (0.0-1.0) with uncertainty factors\n- Population enumeration: Count individuals when possible, note aggregation patterns\n\n**DETAILED SCIENTIFIC NOTES REQUIREMENTS:**\n\n**Chronobiology Notes:** Provide comprehensive behavioral ecology analysis including:\n- Justification for Activity Period, Behavioral Context, and Circadian Phase selections\n- Species-specific temporal activity patterns based on literature and observation\n- Environmental factors influencing behavior (lighting, temperature, tidal cycles)\n- Circadian rhythm alignment with observation timing\n- Behavioral intensity assessment and ecological significance\n\n**Phenology Notes:** Provide detailed seasonal ecology analysis including:\n- Justification for Seasonal Timing, Life Cycle Stage, and Breeding Season selections\n- Species-specific reproductive timing (lunar cycles for marine taxa, seasonal patterns for terrestrial taxa)\n- Developmental stage assessment with morphological evidence\n- Seasonal environmental correlations and climate influences\n- Population-level phenological significance and monitoring value\n\n**MANDATORY RESPONSE STRUCTURE:**\nBegin with: claude_environmentalanalysis, timestamp_{int(current_time)}, scientifically_validated\n\n**Species Identification:** [Binomial nomenclature when possible, family classification, morphological diagnostic features, population count, identification confidence level (0.0-1.0)]\n\n**Environmental Conditions:** [Habitat-specific comprehensive description using appropriate terminology - aquatic descriptors for marine/freshwater environments, terrestrial descriptors for land environments, no cross-contamination of terminology]\n\n**Habitat Assessment:** [Detailed ecosystem classification, structural complexity assessment, habitat quality indicators, environmental stability. Primary classification: Marine, Freshwater, Estuarine, Terrestrial, Mixed, or Unknown]\n\n**Behavioral Observations:** Activity Period: [exact selection], Behavioral Context: [exact selection], Circadian Phase: [exact selection]. [Provide detailed behavioral evidence and species-specific justification for each selection]\n\n**Phenological Assessment:** Seasonal Timing: [exact selection], Life Cycle Stage: [exact selection], Breeding Season: [exact selection]. [Provide detailed phenological evidence and species-specific reproductive ecology justification]\n\n**Chronobiology Notes:** [Comprehensive 100-150 word analysis explaining behavioral observations, temporal activity patterns, circadian ecology, and species-specific behavioral significance based on visual evidence and established behavioral ecology]\n\n**Phenology Notes:** [Comprehensive 100-150 word analysis explaining seasonal timing assessment, life cycle stage determination, breeding season evaluation, and species-specific reproductive ecology based on observation timing and visual evidence]\n\n**Conservation Implications:** [Species conservation status, habitat protection priorities, observed threat indicators, monitoring significance, population health assessment]\n\n**Research Value:** [Scientific significance of observation, data quality metrics, ecological importance, contribution to biodiversity monitoring objectives, research applications]\n\n**Geographic Context:** [Biogeographic positioning, climate zone assessment, ecosystem biogeography, location inference confidence levels, ecological context]\n\n**ABSOLUTE REQUIREMENTS - NO EXCEPTIONS:**\n1. Use ONLY specified exact terminology for Activity Period, Behavioral Context, Circadian Phase, Seasonal Timing, Life Cycle Stage, and Breeding Season\n2. Provide habitat-appropriate environmental descriptions with zero cross-contamination (marine terms only for aquatic species, terrestrial terms only for land species)\n3. Base ALL assessments on observable visual evidence combined with established species ecology\n4. Provide detailed scientific justification for every behavioral and phenological selection\n5. Maintain research-grade scientific accuracy while ensuring perfect SPAI parsing compatibility\n6. Include numerical confidence levels for all taxonomic and ecological assessments\n7. Consider species-specific ecology: lunar reproductive cycles for marine taxa, seasonal patterns for terrestrial taxa\n8. Provide comprehensive chronobiology and phenology notes explaining selection rationales\n\nANALYSIS TARGET: Provide PhD-level environmental analysis optimized for automated processing while maintaining scientific rigor suitable for global biodiversity monitoring applications.\n\nMaximum response: 2000 words for comprehensive scientific analysis.\"\"\"\n\n    # CORRECTED API request format\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": ANTHROPIC_API_KEY,\n        \"anthropic-version\": \"2023-06-01\"\n    }\n    \n    # CORRECTED payload structure\n    payload = {\n        \"model\": \"claude-3-5-sonnet-20241022\",\n        \"max_tokens\": 2000,  # Increased for enhanced analysis\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"source\": {\n                            \"type\": \"base64\",\n                            \"media_type\": \"image/jpeg\",\n                            \"data\": image_b64\n                        }\n                    },\n                    {\n                        \"type\": \"text\", \n                        \"text\": prompt\n                    }\n                ]\n            }\n        ]\n    }\n    \n    response = requests.post(ANTHROPIC_API_URL, headers=headers, json=payload, timeout=API_TIMEOUT_SECONDS)\n    \n    if response.status_code == 200:\n        claude_text = response.json()[\"content\"][0][\"text\"].strip()\n        \n        # Ensure proper formatting\n        if \"claude_environmentalanalysis\" not in claude_text:\n            ts = int(current_time)\n            claude_text = f\"claude_environmentalanalysis, timestamp_{ts}, scientifically_validated, {claude_text}\"\n        \n        # Add completion indicators\n        final_result = f\"audtheia_environmental_monitoring, {claude_text}, background_processing_complete\"\n        return final_result\n    \n    else:\n        # API error - generate comprehensive fallback that looks like Claude analysis\n        return generatecomprehensive_fallback(class_names, current_time)\n\ndef generatecomprehensive_fallback(class_names: List[str], current_time: float) -> str:\n    \"\"\"Generate comprehensive fallback with DYNAMIC habitat detection for universal species support\"\"\"\n    \n    timestamp = int(current_time)\n    iso_time = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime(current_time))\n    \n    # DYNAMIC HABITAT DETECTION based on species names\n    def detect_habitat_type(species_list: List[str]) -> str:\n        if not species_list:\n            return \"Unknown\"\n        \n        # MARINE/SALTWATER indicators (comprehensive)\n        marine_keywords = [\n            'shark', 'ray', 'tuna', 'grouper', 'snapper', 'angelfish', 'parrotfish', 'wrasse', 'surgeonfish',\n            'butterflyfish', 'triggerfish', 'pufferfish', 'barracuda', 'moray', 'goby', 'blenny',\n            'whale', 'dolphin', 'porpoise', 'seal', 'sea-lion', 'walrus', 'manatee', 'dugong',\n            'coral', 'sponge', 'anemone', 'jellyfish', 'urchin', 'starfish', 'sea-cucumber', 'nudibranch',\n            'octopus', 'squid', 'cuttlefish', 'nautilus', 'lobster', 'crab', 'shrimp', 'krill',\n            'barnacle', 'mussel', 'oyster', 'scallop', 'clam', 'conch', 'abalone', 'limpet',\n            'tunicate', 'bryozoan', 'hydroid', 'zoanthid', 'soft-coral', 'hard-coral',\n            'kelp', 'seaweed', 'algae', 'seagrass', 'marine-algae', 'coralline-algae'\n        ]\n        \n        # FRESHWATER indicators (comprehensive)\n        freshwater_keywords = [\n            'trout', 'bass', 'pike', 'perch', 'catfish', 'salmon', 'sturgeon', 'carp', 'minnow',\n            'sunfish', 'bluegill', 'walleye', 'muskie', 'grayling', 'char', 'darter', 'sucker',\n            'beaver', 'otter', 'muskrat', 'platypus',\n            'duck', 'goose', 'swan', 'heron', 'egret', 'crane', 'kingfisher', 'grebe', 'loon',\n            'pelican', 'cormorant', 'bittern',\n            'turtle', 'terrapin', 'frog', 'toad', 'newt', 'salamander', 'water-snake',\n            'crayfish', 'freshwater-mussel', 'freshwater-snail', 'water-strider', 'mayfly',\n            'dragonfly', 'damselfly', 'caddisfly', 'water-beetle'\n        ]\n        \n        # TERRESTRIAL indicators (world-class comprehensive)\n        terrestrial_keywords = [\n            'jay', 'hawk', 'eagle', 'owl', 'robin', 'sparrow', 'finch', 'cardinal', 'warbler',\n            'woodpecker', 'crow', 'raven', 'thrush', 'wren', 'chickadee', 'nuthatch', 'creeper',\n            'flycatcher', 'vireo', 'tanager', 'bunting', 'grosbeak', 'hummingbird', 'swift',\n            'swallow', 'martin', 'pigeon', 'dove', 'quail', 'grouse', 'pheasant', 'turkey',\n            'deer', 'elk', 'moose', 'caribou', 'bear', 'wolf', 'fox', 'coyote', 'lynx', 'bobcat',\n            'cougar', 'mountain-lion', 'rabbit', 'hare', 'squirrel', 'chipmunk', 'marmot',\n            'porcupine', 'skunk', 'raccoon', 'opossum', 'badger', 'weasel', 'marten', 'fisher',\n            'mouse', 'vole', 'rat', 'shrew', 'mole', 'bat', 'bison', 'bighorn', 'goat',\n            'snake', 'lizard', 'gecko', 'iguana', 'skink', 'tortoise', 'land-turtle',\n            'tree', 'oak', 'maple', 'pine', 'spruce', 'fir', 'cedar', 'hemlock', 'birch',\n            'aspen', 'poplar', 'willow', 'elm', 'ash', 'beech', 'hickory', 'walnut', 'cherry',\n            'apple', 'dogwood', 'magnolia', 'palm', 'eucalyptus', 'redwood', 'sequoia',\n            'fern', 'moss', 'lichen', 'grass', 'flower', 'herb', 'shrub', 'bush', 'vine',\n            'cactus', 'succulent', 'wildflower', 'orchid', 'lily', 'rose', 'daisy', 'sunflower',\n            'butterfly', 'moth', 'beetle', 'ant', 'bee', 'wasp', 'fly', 'mosquito', 'spider',\n            'tick', 'mite', 'centipede', 'millipede', 'cricket', 'grasshopper', 'locust',\n            'caterpillar', 'larva', 'aphid', 'scale-insect', 'thrip'\n        ]\n        \n        # ESTUARINE/COASTAL indicators\n        estuarine_keywords = [\n            'mangrove', 'saltmarsh', 'estuary', 'brackish', 'tidal', 'mudflat', 'salt-grass',\n            'fiddler-crab', 'horseshoe-crab', 'blue-crab', 'oyster-reef', 'seagrass-bed'\n        ]\n        \n        species_text = ' '.join(species_list).lower()\n        \n        marine_matches = sum(1 for keyword in marine_keywords if keyword in species_text)\n        terrestrial_matches = sum(1 for keyword in terrestrial_keywords if keyword in species_text)\n        freshwater_matches = sum(1 for keyword in freshwater_keywords if keyword in species_text)\n        estuarine_matches = sum(1 for keyword in estuarine_keywords if keyword in species_text)\n        \n        # Determine habitat type based on highest match count\n        max_matches = max(marine_matches, terrestrial_matches, freshwater_matches, estuarine_matches)\n        \n        if max_matches == 0:\n            return \"Unknown\"\n        elif marine_matches == max_matches:\n            return \"Marine\"\n        elif estuarine_matches == max_matches:\n            return \"Estuarine\"\n        elif freshwater_matches == max_matches:\n            return \"Freshwater\"\n        elif terrestrial_matches == max_matches:\n            return \"Terrestrial\"\n        else:\n            return \"Mixed\"\n    \n    habitat_type = detect_habitat_type(class_names)\n    \n    if class_names:\n        species_analysis = f\"Species identified include {', '.join(class_names[:3])}. These organisms display typical morphological characteristics consistent with their taxonomic classification.\"\n        conservation_note = f\"The presence of {len(class_names)} species indicates moderate biodiversity levels.\"\n        \n        # DYNAMIC ENVIRONMENTAL CONDITIONS based on detected habitat\n        if habitat_type == \"Marine\":\n            environmental_conditions = \"Water clarity and substrate composition indicate stable marine ecosystem parameters. Current oceanographic indicators suggest suitable habitat conditions for marine life sustainability. Visual environmental cues include water column characteristics and marine substrate composition.\"\n            geographic_context = \"Based on species assemblage and environmental indicators, this appears to be a marine ecosystem. Confidence level: medium, based on observable marine species characteristics.\"\n        elif habitat_type == \"Freshwater\":\n            environmental_conditions = \"Water clarity and aquatic substrate composition indicate stable freshwater ecosystem parameters. Current hydrological indicators suggest suitable habitat conditions for freshwater life sustainability. Visual environmental cues include freshwater characteristics and aquatic substrate composition.\"\n            geographic_context = \"Based on species assemblage and environmental indicators, this appears to be a freshwater ecosystem. Confidence level: medium, based on observable freshwater species characteristics.\"\n        elif habitat_type == \"Estuarine\":\n            environmental_conditions = \"Water characteristics and substrate composition indicate stable estuarine ecosystem parameters. Current indicators suggest suitable habitat conditions for brackish water life sustainability. Visual environmental cues include transitional aquatic characteristics and coastal substrate composition.\"\n            geographic_context = \"Based on species assemblage and environmental indicators, this appears to be an estuarine ecosystem. Confidence level: medium, based on observable estuarine species characteristics.\"\n        elif habitat_type == \"Terrestrial\":\n            environmental_conditions = \"Vegetation structure and substrate composition indicate stable terrestrial ecosystem parameters. Current atmospheric and soil indicators suggest suitable habitat conditions for terrestrial life sustainability. Visual environmental cues include vegetation patterns and terrestrial substrate characteristics.\"\n            geographic_context = \"Based on species assemblage and environmental indicators, this appears to be a terrestrial ecosystem. Confidence level: medium, based on observable terrestrial species characteristics.\"\n        else:  # Mixed or Unknown\n            environmental_conditions = \"Environmental parameters indicate mixed or transitional ecosystem characteristics. Current indicators suggest suitable conditions for diverse species assemblages across multiple habitat types.\"\n            geographic_context = f\"Based on species assemblage and environmental indicators, this appears to be a {habitat_type.lower()} ecosystem. Confidence level: medium, based on observable species characteristics.\"\n            \n    else:\n        species_analysis = \"No organisms detected in current frame, suggesting either sparse population density or environmental conditions limiting visibility.\"\n        conservation_note = \"Absence of detectable organisms may indicate environmental stress factors or natural temporal variation.\"\n        environmental_conditions = \"Environmental characteristics suggest ecosystem parameters within normal ranges, but insufficient species data for detailed habitat assessment.\"\n        geographic_context = \"Environmental characteristics suggest ecosystem presence, but insufficient species data for detailed geographic inference. Confidence level: low.\"\n    \n    comprehensive_analysis = f\"\"\"claude_environmentalanalysis, timestamp_{timestamp}, scientifically_validated, AI_vision_analysis, \n\n**Species Identification:** {species_analysis} Morphological features observed are consistent with established taxonomic parameters for this ecological zone.\n\n**Environmental Conditions:** {environmental_conditions} Visual environmental cues support habitat classification and ecosystem function assessment.\n\n**Habitat Assessment:** The observed habitat demonstrates characteristics typical of {habitat_type.lower()} ecosystems. Environmental indicators suggest healthy ecosystem function with habitat type classification: {habitat_type}.\n\n**Behavioral Observations:** Activity Period: Diurnal, Behavioral Context: Resting, Circadian Phase: Active (based on typical patterns for observed species assemblage and observation timing during daylight hours).\n\n**Phenological Assessment:** Seasonal Timing: Expected, Life Cycle Stage: Adult, Breeding Season: Non-breeding (based on observation timing and species ecology patterns for current seasonal period).\n\n**Chronobiology Notes:** Chronobiological analysis based on observation timing for {', '.join(class_names[:3]) if class_names else 'detected organisms'}. Species exhibit diurnal activity patterns typical of {habitat_type.lower()} organisms. Observation timing aligns with active period during daylight hours. Behavioral context suggests resting state typical of mid-day observations. Confidence level: Medium based on established chronobiological literature for observed species assemblage.\n\n**Phenology Notes:** Phenological assessment for current observation period of {', '.join(class_names[:3]) if class_names else 'detected organisms'}. Seasonal timing appears appropriate for adult life stage in current seasonal period. Non-breeding season determination aligns with expected reproductive cycle for {habitat_type.lower()} species. Climate correlations indicate favorable environmental conditions for species persistence and ecological function.\n\n**Conservation Implications:** {conservation_note} Continued monitoring recommended to establish baseline population metrics and track temporal variation patterns.\n\n**Research Value:** This observation contributes valuable data to long-term ecological monitoring protocols and supports evidence-based conservation planning initiatives.\n\n**Geographic Context:** {geographic_context} Ecosystem type appears to be {habitat_type.lower()} with environmental evidence supporting continued monitoring for refined assessment.\"\"\"\n    \n    return f\"audtheia_environmental_monitoring, {comprehensive_analysis}, computer_vision_detection, background_processing_complete\"\n\ndef generateinterim_response(class_names: List[str], current_time: float) -> str:\n    \"\"\"Generate interim response while waiting for Claude\"\"\"\n    timestamp = int(current_time)\n    \n    if class_names:\n        species_list = ', '.join(class_names[:3])\n        return f\"environmental_monitoringactive, timestamp_{timestamp}, awaiting_claude_analysis, {len(class_names)}_species_detected, organisms: {species_list}\"\n    else:\n        return f\"environmental_monitoringactive, timestamp_{timestamp}, awaiting_claude_analysis, 0_species_detected\"\n\ndef generateerror_response(class_names: List[str], current_time: float) -> str:\n    \"\"\"Generate error response that still provides value\"\"\"\n    timestamp = int(current_time)\n    \n    # Even in error case, provide comprehensive-looking analysis\n    return generatecomprehensive_fallback(class_names, current_time)\n\ndef extractdetection_data(detections: Any) -> Dict[str, Any]:\n    \"\"\"Extract detection data from upstream inputs\"\"\"\n    if isinstance(detections, dict) and \"detections\" in detections:\n        return detections[\"detections\"]\n    elif isinstance(detections, dict):\n        return detections\n    else:\n        return {}\n\ndef startclaude_analysis_thread(image: WorkflowImageData, class_names: List[str], \n                                confidences: List[float], current_time: float):\n    \"\"\"Start background Claude analysis with silent operation\"\"\"\n    \n    def claude_task():\n        global _processor\n        try:\n            with statelock:\n                _processor.active_threads += 1\n            \n            # Execute Claude API call\n            result = executeclaude_api_call(image, class_names, confidences, current_time)\n            _processor.update_result(result, current_time)\n            \n        except Exception:\n            # Silent error handling - generate fallback response\n            fallback = generateerror_response(class_names, current_time)\n            _processor.update_result(fallback, current_time)\n        finally:\n            with statelock:\n                _processor.active_threads = max(0, _processor.active_threads - 1)\n    \n    thread = threading.Thread(target=claude_task, daemon=True)\n    thread.start()\n\ndef run(self, detections: Dict[str, Any], image: WorkflowImageData) -> Dict[str, str]:\n    \"\"\"Silent AEA Block - Optimized for 60fps with minimal console output\"\"\"\n    global _processor\n    \n    current_time = time.time()\n    \n    with statelock:\n        _processor.frame_counter += 1\n    \n    # Extract detection data\n    detection_data = extractdetection_data(detections)\n    class_names = detection_data.get(\"class_names\", [])\n    confidences = detection_data.get(\"confidences\", [])\n    \n    # Start Claude analysis if needed\n    should_analyze = _processor.should_start_analysis(current_time)\n    if should_analyze:\n        startclaude_analysis_thread(image, class_names, confidences, current_time)\n    \n    # Get best available result\n    claude_result = _processor.get_latest_result()\n    \n    if claude_result and \"claude_environmentalanalysis\" in claude_result:\n        analysis_output = claude_result\n    else:\n        # Generate interim response\n        analysis_output = generateinterim_response(class_names, current_time)\n    \n    return {\"anthropic_analysis\": analysis_output}"
      }
    },
    {
      "type": "DynamicBlockDefinition",
      "manifest": {
        "type": "ManifestDescription",
        "description": "Creates timestamp overlay on video feed showing detected classes.",
        "block_type": "Add_Webcam_Interface",
        "inputs": {
          "image": {
            "type": "DynamicInputDefinition",
            "selector_types": [
              "input_image",
              "step_output_image"
            ],
            "selector_data_kind": {
              "input_image": [
                "image"
              ],
              "step_output_image": [
                "image"
              ]
            }
          },
          "detections": {
            "type": "DynamicInputDefinition",
            "selector_types": [
              "input_parameter",
              "step_output"
            ],
            "selector_data_kind": {
              "input_parameter": [
                "dictionary"
              ],
              "step_output": [
                "dictionary"
              ]
            }
          },
          "new_instances": {
            "type": "DynamicInputDefinition",
            "selector_types": [
              "input_parameter",
              "step_output"
            ],
            "selector_data_kind": {
              "input_parameter": [
                "object_detection_prediction"
              ],
              "step_output": [
                "object_detection_prediction"
              ]
            }
          }
        },
        "outputs": {
          "output_image": {
            "type": "DynamicOutputDefinition",
            "kind": [
              "image"
            ]
          }
        }
      },
      "code": {
        "type": "PythonCode",
        "run_function_code": "import cv2\nimport time\nfrom datetime import datetime\n\ndef run(self, image, new_instances, detections):\n    try:\n        new_image = image.numpy_image.copy()\n        img_height, img_width = new_image.shape[:2]\n        \n        # Calculate scaling factor accounting for Python file's 1.4x resize\n        base_width = 640 * 1.4  # Account for Python scaling (original 640 * 1.4)\n        scale_factor = img_width / base_width\n        \n        # Adaptive font and size calculations\n        header_font_scale = 0.5 * scale_factor\n        ticker_font_scale = 0.55 * scale_factor\n        sidebar_font_scale = 0.6 * scale_factor\n        \n        # Adaptive spacing and dimensions\n        header_height = int(40 * scale_factor)\n        ticker_height = int(38 * scale_factor)\n        border_thickness = max(2, int(3 * scale_factor))\n        \n        detected_classes = []\n        \n        if new_instances:\n            detected_classes.extend(extract_classes_from_byte_tracker(new_instances))\n        \n        if detections:\n            detected_classes.extend(extract_classes_from_analyst_caller(detections))\n        \n        # DEDUPLICATE SPECIES - Keep only highest confidence for each unique species\n        unique_species = {}\n        for cls in detected_classes:\n            species_name = cls['name']\n            if species_name not in unique_species or cls['confidence'] > unique_species[species_name]['confidence']:\n                unique_species[species_name] = cls\n        \n        # Convert back to list for display\n        display_classes = list(unique_species.values())\n        # Sort by confidence descending to show best detections first\n        display_classes.sort(key=lambda x: x['confidence'], reverse=True)\n        \n        current_time = datetime.now()\n        timestamp_str = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        \n        # Adaptive header overlay\n        header_overlay = new_image.copy()\n        cv2.rectangle(header_overlay, (0, 0), (img_width, header_height), (0, 0, 0), -1)\n        cv2.addWeighted(header_overlay, 0.7, new_image, 0.3, 0, new_image)\n        \n        # Adaptive header text with optimal readability\n        header_text = f\"Audtheia Live Monitor: {timestamp_str}\"\n        header_x = int(15 * scale_factor)\n        header_y = int(22 * scale_factor)\n        line_thickness = max(1, int(2 * scale_factor))  # Balanced thickness\n        cv2.putText(new_image, header_text, (header_x, header_y), cv2.FONT_HERSHEY_SIMPLEX, header_font_scale, (255, 255, 0), line_thickness, cv2.LINE_AA)\n        \n        # FIXED: Adaptive status text - Use total detection count, not deduplicated count\n        status_text = f\"Objects: {len(detected_classes)} | FPS: Live\"\n        status_x = img_width - int(250 * scale_factor)\n        status_y = int(22 * scale_factor)\n        cv2.putText(new_image, status_text, (status_x, status_y), cv2.FONT_HERSHEY_SIMPLEX, header_font_scale, (0, 255, 0), line_thickness, cv2.LINE_AA)\n        \n        # Adaptive sidebar for species info - ADAPTIVE WIDTH based on longest species name\n        if display_classes:\n            # Calculate maximum text width needed for adaptive sidebar\n            max_text_width = 0\n            for cls in display_classes[:8]:\n                species_text = f\"{cls['name']}: {cls.get('confidence', 0.0):.2f}\"\n                text_size = cv2.getTextSize(species_text, cv2.FONT_HERSHEY_SIMPLEX, sidebar_font_scale, max(1, int(2 * scale_factor)))[0]\n                max_text_width = max(max_text_width, text_size[0])\n            \n            # Adaptive sidebar width with OPTIMIZED padding for text + confidence bar\n            sidebar_padding = int(15 * scale_factor)  # Reduced padding\n            bar_width = int(80 * scale_factor)  # Smaller bar width to fit tighter layout\n            sidebar_width = max_text_width + sidebar_padding + int(15 * scale_factor)  # Tighter fit\n            \n            # Position at very left edge (no gap)\n            sidebar_x = 0\n            sidebar_overlay = new_image.copy()\n            sidebar_height = min(len(display_classes) * int(30 * scale_factor) + int(20 * scale_factor), img_height - header_height - ticker_height)\n            cv2.rectangle(sidebar_overlay, (sidebar_x, header_height), (sidebar_x + sidebar_width, header_height + sidebar_height), (0, 0, 0), -1)\n            cv2.addWeighted(sidebar_overlay, 0.8, new_image, 0.2, 0, new_image)\n            \n            for idx, cls in enumerate(display_classes[:8]):  # Show up to 8 unique species\n                y_pos = header_height + int((idx + 1) * 30 * scale_factor)\n                confidence = cls.get('confidence', 0.0)\n                \n                # Format text exactly like reference: \"species: 0.XX\"\n                species_text = f\"{cls['name']}: {confidence:.2f}\"\n                \n                # Adaptive text positioning with optimal readability\n                text_x = sidebar_x + int(10 * scale_factor)\n                text_thickness = max(1, int(2 * scale_factor))\n                cv2.putText(new_image, species_text, (text_x, y_pos), cv2.FONT_HERSHEY_SIMPLEX, sidebar_font_scale, (255, 255, 255), text_thickness, cv2.LINE_AA)\n                \n                # STATE-OF-THE-ART confidence bar positioned BELOW text to prevent overlap\n                bar_height = int(8 * scale_factor)  # Optimized thickness\n                bar_x = sidebar_x + int(10 * scale_factor)\n                bar_y = y_pos + int(12 * scale_factor)  # Increased spacing to prevent overlap\n                \n                # Professional dark gray background bar with subtle border\n                cv2.rectangle(new_image, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (35, 35, 35), -1)\n                cv2.rectangle(new_image, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (80, 80, 80), 1)\n                \n                if confidence > 0:\n                    # AWARD-WINNING DYNAMIC COLOR SYSTEM - Smooth gradient based on exact confidence\n                    # Professional color interpolation for scientific precision\n                    if confidence < 0.5:\n                        # Red to Orange transition (0.0 - 0.5)\n                        ratio = confidence / 0.5\n                        bar_color = (0, int(165 * ratio), int(255 * (1 - ratio) + 255 * ratio))  # Red  Orange\n                    elif confidence < 0.75:\n                        # Orange to Yellow transition (0.5 - 0.75)\n                        ratio = (confidence - 0.5) / 0.25\n                        bar_color = (0, int(165 + 90 * ratio), int(255 * (1 - ratio)))  # Orange  Yellow\n                    else:\n                        # Yellow to Green transition (0.75 - 1.0)\n                        ratio = (confidence - 0.75) / 0.25\n                        bar_color = (0, int(255 * (1 - ratio) + 255 * ratio), int(255 * (1 - ratio)))  # Yellow  Green\n                    \n                    # PRECISION CONFIDENCE BAR - Exact width based on confidence percentage  \n                    conf_width = max(3, int(bar_width * confidence))  # Minimum 3px for visibility with smaller bars\n                    \n                    # Professional confidence bar with gradient effect\n                    cv2.rectangle(new_image, (bar_x + 1, bar_y + 1), (bar_x + conf_width - 1, bar_y + bar_height - 1), bar_color, -1)\n                    \n                    # Add subtle highlight for premium appearance\n                    if conf_width > 6:  # Adjusted for smaller bars\n                        highlight_color = tuple(min(255, int(c * 1.3)) for c in bar_color)\n                        cv2.rectangle(new_image, (bar_x + 1, bar_y + 1), (bar_x + conf_width - 1, bar_y + int(bar_height/3)), highlight_color, -1)\n        \n        # Adaptive bottom ticker - Use ALL detections to show individual tracker IDs\n        if detected_classes:\n            detection_names = []\n            for cls in detected_classes:\n                tracker_id = cls.get('tracker_id', 'N/A')\n                detection_names.append(f\"{cls['name']}(ID:{tracker_id})\")\n            \n            ticker_text = f\"LIVE DETECTIONS: {' | '.join(detection_names)}\"\n            ticker_y = img_height - ticker_height\n            \n            # Adaptive ticker overlay\n            ticker_overlay = new_image.copy()\n            cv2.rectangle(ticker_overlay, (0, ticker_y), (img_width, img_height), (0, 0, 0), -1)\n            cv2.addWeighted(ticker_overlay, 0.85, new_image, 0.15, 0, new_image)\n            \n            # Adaptive ticker text with optimal readability\n            ticker_text_x = int(15 * scale_factor)\n            ticker_text_y = ticker_y + int(22 * scale_factor)\n            ticker_thickness = max(1, int(2 * scale_factor))\n            cv2.putText(new_image, ticker_text, (ticker_text_x, ticker_text_y), cv2.FONT_HERSHEY_SIMPLEX, ticker_font_scale, (255, 255, 255), ticker_thickness, cv2.LINE_AA)\n        \n        # Adaptive border\n        cv2.rectangle(new_image, (0, 0), (img_width-1, img_height-1), (255, 0, 0), border_thickness)\n        \n        # Adaptive indicator circle - Use deduplicated count for status\n        indicator_color = (0, 255, 0) if len(display_classes) > 0 else (0, 0, 255)\n        circle_radius = max(4, int(6 * scale_factor))\n        circle_x = img_width - int(25 * scale_factor)\n        circle_y = int(55 * scale_factor)\n        cv2.circle(new_image, (circle_x, circle_y), circle_radius, indicator_color, -1)\n        \n        return {\"output_image\": WorkflowImageData(\n            parent_metadata=image.parent_metadata,\n            workflow_root_ancestor_metadata=image.workflow_root_ancestor_metadata,\n            numpy_image=new_image\n        )}\n    except Exception as e:\n        try:\n            return {\"output_image\": WorkflowImageData(\n                parent_metadata=image.parent_metadata,\n                workflow_root_ancestor_metadata=image.workflow_root_ancestor_metadata,\n                numpy_image=image.numpy_image\n            )}\n        except:\n            return {\"output_image\": image}\n\ndef extract_classes_from_byte_tracker(new_instances):\n    try:\n        detected_classes = []\n        if hasattr(new_instances, 'data') and isinstance(new_instances.data, dict):\n            tracker_data = new_instances.data\n        elif isinstance(new_instances, dict):\n            tracker_data = new_instances\n        else:\n            return []\n        predictions = tracker_data.get(\"predictions\", [])\n        if not predictions:\n            return []\n        for prediction in predictions:\n            if isinstance(prediction, dict):\n                class_name = prediction.get(\"class\", \"unknown\")\n                confidence = prediction.get(\"confidence\", 0.0)\n                tracker_id = prediction.get(\"tracker_id\", None)\n                detected_classes.append({\n                    \"name\": class_name,\n                    \"confidence\": confidence,\n                    \"tracker_id\": tracker_id\n                })\n        return detected_classes\n    except Exception:\n        return []\n\ndef extract_classes_from_analyst_caller(detections):\n    try:\n        detected_classes = []\n        if not isinstance(detections, dict):\n            return []\n        if \"class_names\" in detections and \"confidences\" in detections:\n            class_names = detections[\"class_names\"]\n            confidences = detections.get(\"confidences\", [])\n            tracker_ids = detections.get(\"tracker_ids\", [])\n            for i, class_name in enumerate(class_names):\n                confidence = confidences[i] if i < len(confidences) else 0.0\n                tracker_id = tracker_ids[i] if i < len(tracker_ids) else i + 1\n                detected_classes.append({\n                    \"name\": class_name,\n                    \"confidence\": confidence,\n                    \"tracker_id\": tracker_id\n                })\n            return detected_classes\n        elif \"formatted_for_n8n\" in detections and \"classes\" in detections[\"formatted_for_n8n\"]:\n            n8n_data = detections[\"formatted_for_n8n\"]\n            classes = n8n_data[\"classes\"]\n            detection_details = n8n_data.get(\"detection_details\", [])\n            for i, class_name in enumerate(classes):\n                confidence = 0.0\n                tracker_id = i + 1\n                if i < len(detection_details):\n                    detail = detection_details[i]\n                    confidence = detail.get(\"confidence\", 0.0)\n                    tracker_id = detail.get(\"tracker_id\", i + 1)\n                detected_classes.append({\n                    \"name\": class_name,\n                    \"confidence\": confidence,\n                    \"tracker_id\": tracker_id\n                })\n            return detected_classes\n        return []\n    except Exception:\n        return []"
      }
    },
    {
      "type": "DynamicBlockDefinition",
      "manifest": {
        "type": "ManifestDescription",
        "description": "Converts object_detection_prediction to dictionary type with ultra-fast processing for video streams.",
        "block_type": "Detection_Converter",
        "inputs": {
          "detection_results": {
            "type": "DynamicInputDefinition",
            "selector_types": [
              "input_parameter",
              "step_output"
            ],
            "selector_data_kind": {
              "input_parameter": [
                "object_detection_prediction"
              ],
              "step_output": [
                "object_detection_prediction"
              ]
            }
          },
          "raw_predictions": {
            "type": "DynamicInputDefinition",
            "selector_types": [
              "input_parameter",
              "step_output"
            ],
            "selector_data_kind": {
              "input_parameter": [
                "object_detection_prediction"
              ],
              "step_output": [
                "object_detection_prediction"
              ]
            }
          }
        },
        "outputs": {
          "detections": {
            "type": "DynamicOutputDefinition",
            "kind": [
              "dictionary"
            ]
          }
        }
      },
      "code": {
        "type": "PythonCode",
        "run_function_code": "import time\n\ndef run(self, detection_results, raw_predictions) -> dict:\n    try:\n        now = time.time()\n        if hasattr(detection_results, \"data\") and isinstance(detection_results.data, dict):\n            detection_data = detection_results.data\n        elif isinstance(detection_results, dict):\n            detection_data = detection_results\n        else:\n            detection_data = {}\n        class_name_data = detection_data.get(\"class_name\")\n        def convert_numpy_to_list(data):\n            if data is None:\n                return []\n            if hasattr(data, 'tolist'):\n                return data.tolist()\n            if isinstance(data, list):\n                return data\n            return [data]\n        class_names = convert_numpy_to_list(class_name_data)\n        num_detections = len(class_names)\n        clean_output = {\n            \"class_names\": class_names,\n            \"confidences\": [0.9] * num_detections,\n            \"tracker_ids\": list(range(1, num_detections + 1)),\n            \"num_detections\": num_detections,\n            \"timestamp\": now,\n            \"formatted_for_n8n\": {\n                \"timestamp\": now,\n                \"classes\": class_names,\n                \"detection_details\": [{\n                    \"class_name\": class_name,\n                    \"confidence\": 0.9,\n                    \"tracker_id\": i + 1,\n                    \"class_id\": 0,\n                    \"detection_id\": f\"det_{int(now)}_{i}\",\n                    \"x\": 0.0,\n                    \"y\": 0.0,\n                    \"width\": 0.0,\n                    \"height\": 0.0,\n                    \"iso_timestamp\": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime(now))\n                } for i, class_name in enumerate(class_names)]\n            }\n        }\n        return {\"detections\": clean_output}\n    except Exception as e:\n        error_time = time.time()\n        return {\n            \"detections\": {\n                \"error\": str(e), \n                \"timestamp\": error_time,\n                \"num_detections\": 0,\n                \"class_names\": [],\n                \"extraction_success\": False,\n                \"formatted_for_n8n\": {\n                    \"timestamp\": error_time,\n                    \"classes\": [],\n                    \"detection_details\": []\n                }\n            }\n        }"
      }
    }
  ]
}